import tensorflow as tf
import numpy as np
import os
from collections import deque
from datetime import datetime
import time
import environment


def configure_tensorflow(use_gpu):
    physical_devices = tf.config.experimental.list_physical_devices('GPU')
    print("Dostępne urządzenia GPU:", physical_devices)
    if use_gpu:
        gpus = tf.config.experimental.list_physical_devices('GPU')
        if gpus:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            tf.config.experimental.set_visible_devices(gpus[0], 'GPU')
            print("GPU dostępne i skonfigurowane.")
        else:
            print("Nie znaleziono urządzenia GPU. Uczenie będzie odbywać się na CPU.")
    else:
        tf.config.set_visible_devices([], 'GPU')
        print("Uczenie będzie odbywać się na CPU.")


class QNetwork(tf.keras.Model):
    def __init__(self, state_size, action_size):
        super(QNetwork, self).__init__()
        self.input_layer = tf.keras.layers.Input(shape=(state_size,))
        self.dense1 = tf.keras.layers.Dense(64, activation='relu')
        self.dense2 = tf.keras.layers.Dense(64, activation='relu')
        self.dense3 = tf.keras.layers.Dense(action_size, activation='linear')

    def call(self, x):
        x = tf.nn.relu(self.dense1(x))
        x = tf.nn.relu(self.dense2(x))
        return self.dense3(x)

    def train_step(self, state, action, reward, next_state, done, gamma, learning_rate):
        with tf.GradientTape() as tape:
            Q_values = self(state)
            Q_action = tf.reduce_sum(tf.multiply(Q_values, tf.one_hot(action, self.dense3.units)), axis=1)
            Q_target = reward + (1 - done) * gamma * tf.reduce_max(self(next_state), axis=1)
            loss = tf.reduce_mean(tf.square(Q_action - Q_target))

        gradients = tape.gradient(loss, self.trainable_variables)
        optimizer = tf.optimizers.Adam(learning_rate=learning_rate)
        optimizer.apply_gradients(zip(gradients, self.trainable_variables))
        return loss.numpy()

    def train_step(self, state, action, reward, next_state, done, gamma, learning_rate):
        with tf.GradientTape() as tape:
            Q_values = self(state)
            Q_action = tf.reduce_sum(tf.multiply(Q_values, tf.one_hot(action, self.action_size)), axis=1)
            Q_target = reward + (1 - done) * gamma * tf.reduce_max(self(next_state), axis=1)
            loss = tf.reduce_mean(tf.square(Q_action - Q_target))

        gradients = tape.gradient(loss, self.trainable_variables)
        optimizer = tf.optimizers.Adam(learning_rate=learning_rate)
        optimizer.apply_gradients(zip(gradients, self.trainable_variables))
        return loss.numpy()


class DQNagent:
    def __init__(self):
        self.state_size = 5
        self.action_size = 8   # możliwe akcje, czyli ruchy, 8 możliwych
        self.batch_size = 100
        self.no_episodes = 30
        self.max_memory = 50_000

        self.output_dir = "agent_output/"

        self.memory = deque(maxlen=self.max_memory)
        self.gamma = 0.95
        self.epsilon = 1.0
        self.epsilon_decay = 0.999
        self.epsilon_min = 0.01
        self.learning_rate = 0.001
        self.loss = []
        self.reward_per_episode = 0
        self.accumulative_reward = 0
        self.steps_per_episode = 0
        self.model = self._build_model()

    def _build_model(self):  # Predict future reward using regression for DQN agent.
        return QNetwork(self.state_size, self.action_size)

    def remember(self, state, action, reward, next_state, done):  # done for episode
        self.memory.append((state, action, reward, next_state, done))

    def get_action(self, state):  # Based on epsilon explore randomly or exploit current data.
        if np.random.rand() <= self.epsilon:  # Exploration mode.
            return np.random.choice(self.action_size)
        # print("=====================================================")
        state = np.expand_dims(state, axis=0)
        state_tensor = tf.convert_to_tensor(state, dtype=tf.float32)
        action_values = self.model(state_tensor)  # Exploit, over epsilon decay, more exploration.
        return tf.argmax(action_values[0]).numpy()

    def train_short_memory(self, old_state, action, reward, new_state, done):
        single_minibatch = [(old_state, action, reward, new_state, done)]
        self.train_model(single_minibatch, done)

    def train_long_memory(self):
        if len(self.memory) > self.batch_size:
            minibatch = np.random.sample(self.memory, self.batch_size)
        else:
            minibatch = self.memory
        self.train_model(minibatch, True)

    def train_model(self, minibatch, is_episode_done):
        for batchmini in minibatch:
            state, action, reward, next_state, done = batchmini
            state_tensor = tf.convert_to_tensor(state, dtype=tf.float32)
            next_state_tensor = tf.convert_to_tensor(next_state, dtype=tf.float32)

            with tf.GradientTape() as tape:
                Q_values = self.model(state_tensor)
                Q_action = tf.reduce_sum(tf.multiply(Q_values, tf.one_hot(action, self.action_size)), axis=1)
                Q_target = reward + (1 - done) * self.gamma * tf.reduce_max(self.model(next_state_tensor), axis=1)  # Bellman Equation
                loss = tf.reduce_mean(tf.square(Q_action - Q_target))

            gradients = tape.gradient(loss, self.model.trainable_variables)
            optimizer = tf.optimizers.Adam(learning_rate=self.learning_rate)
            optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))
            self.loss.append(loss.numpy())

        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

    def load(self, name, should_load, agent):
        if should_load:
            if name is not None:
                self.model.load_weights(f"{agent.output_dir}weights/{name}")
            else:
                files = os.listdir(f"{self.output_dir}weights/")
                files = [file for file in files if file.startswith("episode_") and file.endswith("weights.hdf5")]
                if files:
                    latest_file = files[-1]
                    self.model.load_weights(f"{agent.output_dir}weights/{latest_file}")
                else:
                    print("No weight files found.")
        else:
            pass

    def save(self, name):
        self.model.save_weights(name)


def driver():
    # initialize agent and env
    agent = DQNagent()
    agent.load(name=None, should_load=False, agent=agent)
    env = environment.RobotSimulation()

    if not os.path.exists(agent.output_dir):
        os.makedirs(agent.output_dir)

    start_time = datetime.now()
    start_time_str = start_time.strftime("%Y-%m-%d %H:%M:%S")
    with open(f"{agent.output_dir}execution_time.txt", "a") as execution_time_file:
        execution_time_file.write(f"Execution started at: {start_time_str}\n")
    epsilon_values_file = open(f"{agent.output_dir}epsilon_values.txt", "w")
    loss_values_file = open(f"{agent.output_dir}loss_values.txt", "w")
    accumulative_reward_values_file = open(f"{agent.output_dir}accumulative_reward_values.txt", "w")
    reward_values_per_episode_file = open(f"{agent.output_dir}reward_values_per_episode_file.txt", "w")
    no_finished_games_file = open(f"{agent.output_dir}no_finished_games_file.txt", "w")
    steps_per_episode_file = open(f"{agent.output_dir}steps_per_episode_file.txt", "w")

    for episode in range(agent.no_episodes):
        print(f"EPISODE{episode}")

        agent.accumulative_reward += agent.reward_per_episode
        accumulative_reward_values_file.write(
            f"episode - {episode}/{agent.no_episodes}, "f"accumulative_reward = {agent.accumulative_reward}\n")
        reward_values_per_episode_file.write(
            f"episode - {episode}/{agent.no_episodes}, "f"reward_per_episode = {agent.reward_per_episode}\n")
        agent.reward_per_episode = 0
        agent.steps_per_episode = 0

        # Episode lasts until done returns True flag and penalty is given or if aim is reached
        while True:
            # get current step
            old_state = env.get_states()
            # choose action
            action = agent.get_action(old_state)
            print("action = ", action)
            # perform action
            reward, done, episode_finished = env.do_step(action)
            agent.steps_per_episode += 1

            if episode_finished:
                no_finished_games_file.write(f"episode - {episode}/{agent.no_episodes}, STATUS: AIM REACHED\n")

            agent.reward_per_episode += reward
            # get new state after action
            new_state = env.get_states()
            # reshape to fit TensorFlow model input
            new_state = np.reshape(new_state, [1, agent.state_size])
            old_state = np.reshape(old_state, [1, agent.state_size])
            # remember feedback to train deep neural network
            agent.remember(old_state, action, reward, new_state, done)
            # use short memory to train
            agent.train_short_memory(old_state, action, reward, new_state, done)

            if done:
                # use long memory to train
                agent.train_long_memory()
                agent.save(name=f"{agent.output_dir}weights/episode_{episode}_weights.hdf5")
                steps_per_episode_file.write(
                    f"episode - {episode}/{agent.no_episodes}, "f"steps_per_episode = {agent.steps_per_episode}\n")
                loss_values_file.write(f"episode - {episode}/{agent.no_episodes}, loss = {agent.loss[0]}\n")
                epsilon_values_file.write(f"episode - {episode}/{agent.no_episodes}, "f"epsilon = {agent.epsilon}\n")
                # reset env
                env.reset_env()
                # break and take another episode
                break

    end_time = datetime.now()
    end_time_str = end_time.strftime("%Y-%m-%d %H:%M:%S")
    elapsed_time = end_time - start_time
    with open(f"{agent.output_dir}execution_time.txt", "a") as execution_time_file:
        execution_time_file.write(f"Execution finished at: {end_time_str}\n")
        execution_time_file.write(f"Total execution time: {elapsed_time}\n")
        execution_time_file.write("================================================\n")

    execution_time_file.close()
    no_finished_games_file.close()
    loss_values_file.close()
    epsilon_values_file.close()
    accumulative_reward_values_file.close()
    reward_values_per_episode_file.close()
    steps_per_episode_file.close()


if __name__ == "__main__":
    start_time = time.time()
    print("Start")
    configure_tensorflow(use_gpu=False)
    driver()
    end_time = time.time()
    execution_time = end_time - start_time
    print(f"Czas wykonania: {execution_time} sekundy")  # Czas wykonania: 16.360511541366577 sekundy
